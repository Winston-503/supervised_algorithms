## Bagging

**Bagging** stands for *bootstrap aggregating*.

When we have a train set `X_train (N x M) N data points and M features` then we train `n` trees on `X`, where `X (N x M)` is a random subsample of `X_train` with the same size.
When `X` is formed **with replacement** algorithm is called **bagging**, and when `X` is formed **without replacement** algorithm is called **pasting**.
When this model does prediction, really, it gets `n` predictions from `n` different models and aggregates them.
*Classification* is computed from a simple majority vote of the models and *regression* is computed from a mean value of the models' predictions.

| ![bagging.jpg](../img/bagging.jpg) |
|:--:|
| <b>Bagging. Image by Author</b>|


**Pasting** was originally designed for **large datasets**, when computing power is limited. **Bagging**, on the other hand, can use the same subsets many times, which is great for smaller sample sizes, in which it improves robustness.

This approach allows **to leave the same bias, but decrease the variance** thanks to *Central Limit Theorem*.
The more variable the algorithms are, the lower the correlation of their predictions and, accordingly, the CLT works better (decision trees are a great choice).

If we are using bagging, there is a chance that a sample would never be selected, while others may be selected multiple times. In general, for a big dataset, 37% of its samples are never selected and we could use it to test our model. This is called **Out-of-Bag scoring**, or **OOB Scoring**.

Main hyperparameters:
- type of models
- n_estimators - the number of models in the ensemble
- max_samples - the number of samples to take from train set to train each base model
- max_features - the number of features to take from train set to train each base model

Pros:
+ Very good quality
+ Training process can be simply parallelized because models learns independently from each other
+ Does not require features preprocessing and built-in assessment of the importance of features (in case of trees)
+ Resistant to overfitting
+ Resistant to outliers
+ *OOB Scoring* allows to use full dataset without splitting it into train and validation

Cons:
- Complexity of interpretation
- Does not cope well with a very large number of features or for sparse data
- Trains and makes predictions significantly slower than linear models