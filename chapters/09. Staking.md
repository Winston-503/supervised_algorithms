## Staking

The architecture of a stacking model involves **two or more base models**, often referred to as *level-0 models*, and a **meta-model that combines the predictions of the base models**, referred to as a *level-1 model*.
- *Level-0 Models (Base-Models)*: Training data is divided into `K` folds. Then `K` models fit on the `K-1` folds.
- *Level-1 Model (Meta-Model)*: Model that learns how to combine the predictions of the base models in the best possible way.

Differences from boosting:
- **Unlike bagging**, in stacking, the **models are typically different** (e.g. not all decision trees) and **fit on the same dataset** (e.g. instead of samples of the training dataset).
- **Unlike boosting**, in stacking, **a single model is used to learn how to best combine the predictions from the contributing models** (e.g. instead of a sequence of models that correct the predictions of prior models).

The usage of a simple linear model as the meta-model often gives stacking the colloquial name **blending**.

Proc:
+ Improves the quality of the model when nothing else helps
+ Allows you to effectively mix models of different classes, combining their strengths
+ Help you win gold on Kaggle

Cons:
- High computational complexity 
- Complexity of interpretation
- You can easily overfit with leaking information
- Works well only on sufficiently large datasets
