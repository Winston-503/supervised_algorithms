## Boosting

A boosting is **an ensemble of weak algorithms** (the prediction accuracy is slightly better than random) that are trained **sequentially and each subsequent one considers the error of the previous one**.

| ![boosting.jpg](../img/boosting.jpg) |
|:--:|
| <b>Boosting</b>|

Types of Boosting:
- **AdaBoost** (Adaptive Boosting)
  
  A greedy algorithm for constructing a linear combination of algorithms. At each step, large weights are assigned to the incorrectly predicted examples. Sensitive to outliers.

- **GDM - Gradient Boosting Machine** 
  
  At each step, the algorithm is adjusted to minimize the errors made in the previous steps.

- **XGBoost - eXtreme Gradient Boosting** and **LGBM - Light GBM**
  
  - [XGBoost](https://xgboost.readthedocs.io/en/latest/index.html) is an implementation of gradient boosted decision trees designed for speed and performance. 
  - [XGBFIR](https://github.com/limexp/xgbfir) is a great library for XGBoost feature importance analysis
  - [Light GBM](https://lightgbm.readthedocs.io/en/latest/)

Main hyperparameters:
- Types of models and ways of their interaction with each other

Pros:
+ Very good quality, usually better than random forest
+ Built-in assessment of the importance of features

Cons:
- Learning is slower than random forest, because learning process has to be strictly sequential
- Prone to overfitting
- Works well only with sufficiently large datasets