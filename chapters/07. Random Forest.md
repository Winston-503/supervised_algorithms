## Random Forest

Despite the fact that *bagging* can be applied with all types of algorithms, **bagging over decision trees** has become widespread. Since they are unstable and variable, a good result is obtained. In fact, **random forest** is **bagging over decision trees with random subspace method**.

When we have train set `X_train N x M (N data points and M features)` then we train `n` trees on `X`, where `X (N x m)` is random subsample of `X_train` with replacement, but we also take a random subset of the `m (m < M)` features. This is called the *Random Subspace Method*.
When this model does prediction, really, it gets `n` predictions from `n` different models and aggregates them.
*Classification* is computed from a simple majority vote of the models and *regression* is computed from a mean value of the models' predictions.

This approach allows **to leave the same bias, but decrease the variance** thanks to *Central Limit Theorem*.

As known **Isolation Forest** algorithm also can be used for the *Anomaly detection* problem.

**Inherits the pros and cons of bagging**.

Main hyperparameters:
- n_estimators - the number of trees in the ensemble - the more the better
- max_features - the number of features to draw from train set to train each base tree - `n/3` for regression and `sqrt(n)` for classification is recommended
- max_depth - the maximum depth of the tree
- min_sample_leaf - the minimum number of samples required to split an internal node

## Extra Trees

Extra Trees is related to the widely used random forest algorithm. 

- **Unlike bagging and random forest** that trains each decision tree from a bootstrap sample of the training dataset, the *Extra Trees* algorithm trains each decision tree on the **whole training dataset**.
- **Like random forest**, the *Extra Trees* algorithm will **randomly sample the features** at each split point of a decision tree. 
- **Unlike random forest**, which uses a greedy algorithm to select an optimal split point, the *Extra Trees* algorithm **randomly selects a split point**.

It can often achieve **as good or better performance than the random forest algorithm**, although it uses a simpler algorithm to construct the decision trees used as members of the ensemble, so **it works faster**. 

**Inherits all hyperparameters, the pros and cons of random forest**.