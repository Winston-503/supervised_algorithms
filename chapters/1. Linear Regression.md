## Linear Regression

In the simplest case, the regression task is to draw a line through the data points so that an error between this line (predictions) and real values is minimal.
In general, this is the problem of *minimizing the loss function*, so *the optimization problem*. Usually, the loss function is the *MSE - mean square error* (because of *MLE - maximum likelihood estimation*), and the optimization algorithm is *gradient descent*. Anyway, any other loss function of optimization algorithm can be used.

One of the important properties of linear regression is that optimal parameters (according to *MSE because of MLE*) can be calculated with simple **Normal Equation**, but this method does not scale well with large number of features, so any other optimization method can be applied instead.

If the data dependences is more complex, than a straight line, we can add powers of each feature as new features (*PolynomialFeatures* class from *sklearn* can be used) and then train a Linear Regression model. This technique is called **Polynomial Regression**.

Other popular version of this algorithm is **Bayesian Linear Regression**, that predicts not only values, but also it's probabilities, by building a *confidence interval*. This is possible thanks to *Bayes' theorem*.

One of the most efficient way to avoid overfitting and outliers influence with regression is **regularization**. *Regularization term* is added to loss function so regression coefficients have to be as little as possible. 
- **LASSO regression** - implements L1 regularization, + |coeff|.
- **Ridge regression** - implements L2 regularization, + coeff^2. Also known as *Tikhonov regularization*.
- **Elastic Net regression** - implements both L1 and L2 regularization.

As mentioned earlier, *linear regression solve only regression task*. 

Hyperparameters:
- regularization type and parameter
- solver - optimization algorithm

Pros:
+ Have few parameters, and learn fast
+ Can be configured using *stochastic gradient descent*, without the need to store all the samples in memory and can be used for *online learning*
+ More interpretable than complex models
+ Is well suited for problems with a small number of data points and large number of features
+ Is well suited for sparse data

Cons:
- Poorly restores complex dependencies
- Requires data pre-processing 
