## Support Vector Machines

Support Vector Machines algorithm is based on *support vectors* concept - the extreme points.  
In case of *classification task* it tries to draw a separating line between classes such that *support vectors* are located as far as possible from this line (separating hyperplane in general case):
- *Hard Margin Classification* - it is assumed that instances of the same class are on the same side of the separating hyperplane without exceptions.
- *Soft Margin Classification* - allows violation of the decision boundary, which is regulated by the regularization parameter.

In case of *regression task*, instead, we want to draw a line to fit as many as possible instances on the street.

Since SVM requires calculating distances between points it requires *feature scaling*.

The most important and mathematically elegant feature of SVM is that the solution of the *Dual Problem* (to which SVM is reduced) does not depend on the feature descriptions of objects as vectors, but *only on their pairwise scalar products*.
This allows us to replace the scalar product with a certain function *K(a, b)*, which is called the *kernel*. In fact, the kernel is a *scalar product in some other space*. This procedure allows you to build nonlinear classifiers (which are actually linear in a larger dimension space) without adding new features and is called **kernel trick**.

The use of different kernels allows this algorithm to recover very complex dependencies in both *classification* and *regression* tasks. The most popular kernels are:
- polynomial
- RBF - Gaussian Radial Basis Function
- sigmoid and others

One-class SVM also can be used for the *Anomaly Detection* problem.

Hyperparameters:
- kernel type
- regularization parameter - a penalty for each misclassified data point

Pros:
+ One of the most powerful and flexible models
+ Can be used for *online learning*
+ As linear model inherits the pros of linear regression

Cons:
- Requires data preprocessing
- It scales well with number of features, but not samples, so works well only on small and medium-sized datasets